{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22f62e0",
   "metadata": {},
   "source": [
    "# Wav2Vec2 Hindi ASR Model - Interactive Notebook\n",
    "\n",
    "This notebook demonstrates how to use the SPRING-INX Wav2Vec2 model for Hindi speech-to-text transcription.\n",
    "\n",
    "## What we're doing:\n",
    "- Loading a pre-trained Wav2Vec2 model for Hindi\n",
    "- Transcribing audio files from Hindi speech to text\n",
    "- Measuring inference time on CPU (Xeon)\n",
    "\n",
    "Let's get started! üé§‚Üíüìù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ced42",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "882cb118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully!\n",
      "PyTorch version: 2.1.0+cpu\n",
      "Device: CPU\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/ubuntu/ASR/ASR/Wav2Vec2/.venv/lib/python3.10/site-packages')\n",
    "\n",
    "import torch\n",
    "import fairseq\n",
    "import librosa\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.sox_effects as ta_sox\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ee09c",
   "metadata": {},
   "source": [
    "## Step 2: Define the AudioToText Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "054cba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì AudioToText class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class AudioToText:\n",
    "    \"\"\"Wav2Vec2 based Hindi speech-to-text transcriber\"\"\"\n",
    "    \n",
    "    DEFAULT_SAMPLING_RATE = 16000\n",
    "\n",
    "    def __init__(self, model_path=\"hindi.pt\", warmup_iterations=1, sample_audio_path=\"./samples/hindi.wav\"):\n",
    "        \"\"\"\n",
    "        Initialize the ASR model.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the .pt model file\n",
    "            warmup_iterations: Number of warmup iterations\n",
    "            sample_audio_path: Path to a sample audio for warmup\n",
    "        \"\"\"\n",
    "        print(\"Loading model...\")\n",
    "        self.model, self.cfg, self.task = fairseq.checkpoint_utils.load_model_ensemble_and_task([model_path])\n",
    "        self.model = self.model[0]\n",
    "        self.dtype = torch.float32\n",
    "        self.model.to(self.dtype)\n",
    "        self.model.eval()\n",
    "        print(f\"‚úì Model loaded from {model_path}\")\n",
    "\n",
    "        self.effects = [[\"gain\", \"-n\"]]\n",
    "        self.token = self.task.target_dictionary\n",
    "        self.warmup_audio_path = sample_audio_path\n",
    "        \n",
    "        print(f\"Running {warmup_iterations} warmup iteration(s)...\")\n",
    "        self.warmup(warmup_iterations)\n",
    "        print(\"‚úì Model ready for inference!\")\n",
    "    \n",
    "    def warmup(self, warmup_iters: int):\n",
    "        \"\"\"Run warmup iterations to optimize performance\"\"\"\n",
    "        for i in range(warmup_iters):\n",
    "            with torch.no_grad():\n",
    "                _ = self.transcribe(self.warmup_audio_path)\n",
    "            print(f\"  Warmup {i+1}/{warmup_iters} complete\")\n",
    "\n",
    "    def transcribe(self, path, sample_rate=16000):\n",
    "        \"\"\"\n",
    "        Transcribe an audio file to text.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to audio file\n",
    "            sample_rate: Audio sampling rate (default: 16kHz)\n",
    "            \n",
    "        Returns:\n",
    "            transcriptions: List of transcribed text\n",
    "            total_time: Time taken for inference in seconds\n",
    "        \"\"\"\n",
    "        # Load audio\n",
    "        audio, sr = librosa.load(path, sr=sample_rate)\n",
    "        \n",
    "        # Start timing\n",
    "        st = time.perf_counter()\n",
    "        \n",
    "        # Apply effects\n",
    "        input_sample, rate = ta_sox.apply_effects_tensor(\n",
    "            torch.tensor(audio).unsqueeze(0), sample_rate, self.effects)\n",
    "        input_sample = input_sample.to(self.dtype)\n",
    "        \n",
    "        # Normalize\n",
    "        with torch.no_grad():\n",
    "            input_sample = F.layer_norm(input_sample, input_sample.shape)\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(source=input_sample, padding_mask=None)['encoder_out']\n",
    "        \n",
    "        predicted_ids = torch.argmax(logits, axis=-1)\n",
    "        predicted_ids = torch.unique_consecutive(predicted_ids.T, dim=1).tolist()\n",
    "\n",
    "        # Convert token IDs to text\n",
    "        transcriptions = []\n",
    "        for ids in predicted_ids:\n",
    "            transcription = self.token.string(ids)\n",
    "            transcription = transcription.replace(' ', \"\").replace('|', \" \").strip()\n",
    "            transcriptions.append(transcription)\n",
    "        \n",
    "        total_time = time.perf_counter() - st\n",
    "        \n",
    "        return transcriptions, total_time\n",
    "\n",
    "print(\"‚úì AudioToText class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147c325",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90385b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ASR/ASR/Wav2Vec2/.venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded from hindi.pt\n",
      "Running 1 warmup iteration(s)...\n",
      "  Warmup 1/1 complete\n",
      "‚úì Model ready for inference!\n"
     ]
    }
   ],
   "source": [
    "# Create the transcriber instance\n",
    "transcriber = AudioToText(\n",
    "    model_path=\"hindi.pt\",\n",
    "    warmup_iterations=1,\n",
    "    sample_audio_path=\"./samples/hindi.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98805163",
   "metadata": {},
   "source": [
    "## Step 4: Transcribe Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c8378d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing: ./samples/hindi.wav\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription:\n",
      "  ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§≤‡•ã‡§ï‡§≤ ‡§™‡•ç‡§∞‡•ã‡§°‡§ï‡•ç‡§ü ‡§ï‡•ã ‡§ó‡•ç‡§≤‡•ã‡§¨‡§≤ ‡§¨‡§®‡§æ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§π‡§Æ‡§æ‡§∞‡•á ‡§ú‡§Æ‡•ç‡§Æ‡•Ç-‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§ï‡•á ‡§≤‡•ã‡§ó ‡§≠‡•Ä ‡§™‡•Ä‡§õ‡•á ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à ‡§™‡§ø‡§õ‡§≤‡•á ‡§Æ‡§π‡•Ä‡§®‡•á\n",
      "\n",
      "Inference Time: 0.3407 seconds\n"
     ]
    }
   ],
   "source": [
    "# Transcribe the first sample\n",
    "print(\"Transcribing: ./samples/hindi.wav\")\n",
    "print(\"=\" * 50)\n",
    "transcription, inference_time = transcriber.transcribe(\"./samples/hindi.wav\")\n",
    "print(f\"\\nTranscription:\")\n",
    "for text in transcription:\n",
    "    print(f\"  {text}\")\n",
    "print(f\"\\nInference Time: {inference_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de1443",
   "metadata": {},
   "source": [
    "## Step 5: Transcribe Second Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32b82c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing: ./samples/hindi2.wav\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription:\n",
      "  ‡§ú‡§Æ‡•ç‡§Æ‡•Ç-‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§®‡•á ‡§ú‡•ã ‡§ï‡§∞ ‡§¶‡§ø‡§ñ‡§æ‡§Ø‡§æ ‡§π‡•à ‡§µ‡§π ‡§¶‡•á‡§∂ ‡§≠‡§∞ ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≠‡•Ä ‡§è‡§ï ‡§Æ‡§ø‡§∏‡§æ‡§≤ ‡§π‡•à ‡§Ø‡§π‡§æ‡§Ç ‡§ï‡•á ‡§™‡•Å‡§≤‡§µ‡§æ‡§Æ‡§æ ‡§∏‡•á‡•§\n",
      "\n",
      "Inference Time: 0.3313 seconds\n"
     ]
    }
   ],
   "source": [
    "# Transcribe the second sample\n",
    "print(\"Transcribing: ./samples/hindi2.wav\")\n",
    "print(\"=\" * 50)\n",
    "transcription, inference_time = transcriber.transcribe(\"./samples/hindi2.wav\")\n",
    "print(f\"\\nTranscription:\")\n",
    "for text in transcription:\n",
    "    print(f\"  {text}\")\n",
    "print(f\"\\nInference Time: {inference_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb83c47",
   "metadata": {},
   "source": [
    "## Step 6: Batch Transcription (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20334a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Transcription Results\n",
      "==================================================\n",
      "\n",
      "File: ./samples/hindi.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text: ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§≤‡•ã‡§ï‡§≤ ‡§™‡•ç‡§∞‡•ã‡§°‡§ï‡•ç‡§ü ‡§ï‡•ã ‡§ó‡•ç‡§≤‡•ã‡§¨‡§≤ ‡§¨‡§®‡§æ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§π‡§Æ‡§æ‡§∞‡•á ‡§ú‡§Æ‡•ç‡§Æ‡•Ç-‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§ï‡•á ‡§≤‡•ã‡§ó ‡§≠‡•Ä ‡§™‡•Ä‡§õ‡•á ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à ‡§™‡§ø‡§õ‡§≤‡•á ‡§Æ‡§π‡•Ä‡§®‡•á\n",
      "  Time: 0.4361s\n",
      "\n",
      "File: ./samples/hindi2.wav\n",
      "  Text: ‡§ú‡§Æ‡•ç‡§Æ‡•Ç-‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§®‡•á ‡§ú‡•ã ‡§ï‡§∞ ‡§¶‡§ø‡§ñ‡§æ‡§Ø‡§æ ‡§π‡•à ‡§µ‡§π ‡§¶‡•á‡§∂ ‡§≠‡§∞ ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≠‡•Ä ‡§è‡§ï ‡§Æ‡§ø‡§∏‡§æ‡§≤ ‡§π‡•à ‡§Ø‡§π‡§æ‡§Ç ‡§ï‡•á ‡§™‡•Å‡§≤‡§µ‡§æ‡§Æ‡§æ ‡§∏‡•á‡•§\n",
      "  Time: 0.4284s\n",
      "\n",
      "File: ./samples/modi_speech.wav\n",
      "  Text: ‡§∏ ‡§∏‡§æ ‡§•‡§ø ‡§Ø‡•ã  ‡§≤‡•ã‡§ï‡§≤ ‡§™‡•ç‡§∞‡•ã‡§°‡§ï‡•ç‡§ü ‡§ï‡•ã ‡§ó‡•ç‡§≤‡•ã‡§¨‡§≤ ‡§¨‡§®‡§æ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§π‡§Æ‡§æ‡§∞‡•á ‡§ú‡§Æ‡•ç‡§Æ‡•Ç-‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§ï‡•á ‡§≤‡•ã‡§ó ‡§≠‡•Ä ‡§™‡•Ä‡§õ‡•á ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à ‡§™‡§ø‡§õ‡§≤‡•á ‡§Æ‡§π‡•Ä‡§®‡•á ‡§ú‡§Æ‡•ç‡§Æ‡•Ç-‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§®‡•á ‡§ú‡•ã ‡§ï‡§∞ ‡§¶‡§ø‡§ñ‡§æ‡§Ø‡§æ ‡§π‡•à ‡§µ‡•ã ‡§¶‡•á‡§∂ ‡§≠‡§∞ ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≠‡•Ä ‡§è‡§ï ‡§Æ‡§ø‡§∏‡§æ‡§≤ ‡§π‡•à ‡§Ø‡§π‡§æ‡§Ç ‡§ï‡•á ‡§™‡•Å‡§≤‡§µ‡§æ‡§Æ‡§æ ‡§∏‡•á ‡§∏‡•ç‡§®‡•ã ‡§™‡•Ä‡§ú ‡§ï‡•Ä ‡§™‡§π‡§≤‡•Ä ‡§ñ‡•Ä ‡§≤‡§Ç‡§¶‡§® ‡§¨‡•á‡§ú‡•Ä ‡§ó‡§à ‡§ï‡•Å‡§õ ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§Ø‡•á ‡§Ü‡§á‡§°‡§ø‡§Ø‡§æ ‡§∏‡•Ç‡§ù‡§æ ‡§ï‡§ø ‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§Æ‡•á‡§Ç ‡§â‡§ó‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§è‡§ï‡•ç‡§ú‡•ã‡§ü‡•Ä ‡§µ‡•á‡§ú‡§ø‡§ü‡•á‡§¨‡§≤‡•ç‡§∏ ‡§ï‡•ã ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§® ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§®‡§ï‡•ç‡§∏‡•á ‡§™‡§∞ ‡§≤‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§è ‡§¨‡§∏ ‡§´‡§ø‡§∞ ‡§ï‡•ç‡§Ø‡§æ ‡§•‡§æ ‡§ö‡§ï‡•Å‡§∞‡§æ ‡§ó‡§æ‡§Ç‡§µ ‡§ï‡•á ‡§Ö‡§¨‡•ç‡§¶‡•Å‡§≤ ‡§∞‡§æ‡§∏‡•Ä ‡§Æ‡§ø‡§≤ ‡§ú‡•Ä ‡§á‡§∏‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§¨‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§Ü ‡§ó‡§Ø‡§æ‡§è ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§ó‡§æ‡§Ç‡§µ ‡§ï‡•á ‡§Ö‡§®‡•ç‡§Ø ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§ú‡§Æ‡•Ä‡§® ‡§ï‡•ã ‡§è‡§ï ‡§∏‡§æ‡§• ‡§Æ‡§ø‡§≤‡§æ‡§ï‡§∞ ‡§∏‡§®‡•ã ‡§™‡§ú ‡§â‡§ó‡§æ‡§®‡•á ‡§ï‡§æ ‡§ï‡§æ‡§Æ ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§ø‡§Ø‡§æ‡§î‡§∞ ‡§¶‡•á‡§ñ‡§§‡•á ‡§π‡•Ä ‡§¶‡•á‡§ñ‡§§‡•á ‡§∏‡•ç‡§®‡•ã ‡§™‡§ú ‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§∏‡•á ‡§≤‡§Ç‡§¶‡§® ‡§§‡§ï ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•á ‡§≤‡§ó‡•Ä ‡§á‡§∏ ‡§∏‡§´‡§≤‡§§‡§æ ‡§®‡•á ‡§ú‡§Æ‡•ç‡§Æ‡•Ç-‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß‡§ø ‡§ï‡•á ‡§≤‡§ø‡§è ‡§®‡§è ‡§¶‡•ç‡§µ‡§æ‡§∞ ‡§ñ‡•Å‡§≤‡•á‡§π‡§Æ‡§æ‡§∞‡•á ‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§ê‡§∏‡•á ‡§Ø‡•Ç‡§®‡§ø‡§ï ‡§™‡•ç‡§∞‡•ã‡§°‡§ï‡•ç‡§ü ‡§ï‡•Ä ‡§ï‡§Æ‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡§Ü‡§™ ‡§ê‡§∏‡•á ‡§™‡•ç‡§∞‡•ã‡§°‡§ï‡•ç‡§ü‡§∏ ‡§ï‡•ã ‡§π‡•à‡§∂‡•ç‡§ü‡•à‡§ó ‡§Æ‡§æ‡§Ø ‡§™‡•ç‡§∞‡•ã‡§°‡§ï‡•ç ‡§Æ‡§æ‡§Ø ‡§™‡•ç‡§∞‡§æ‡§á‡§° ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡§∞‡•Ç‡§∞ ‡•á ‡§Ø ‡§∞ ‡§ï ‡§∞‡•á   ‡§π‡§æ‡•§\n",
      "  Time: 2.8817s\n",
      "\n",
      "==================================================\n",
      "Batch processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Transcribe multiple files at once\n",
    "from pathlib import Path\n",
    "\n",
    "audio_files = [\n",
    "    \"./samples/hindi.wav\",\n",
    "    \"./samples/hindi2.wav\",\n",
    "    \"./samples/modi_speech.wav\"\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Batch Transcription Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for audio_file in audio_files:\n",
    "    if Path(audio_file).exists():\n",
    "        print(f\"\\nFile: {audio_file}\")\n",
    "        try:\n",
    "            transcription, inference_time = transcriber.transcribe(audio_file)\n",
    "            results[audio_file] = {\n",
    "                'transcription': transcription,\n",
    "                'time': inference_time\n",
    "            }\n",
    "            print(f\"  Text: {transcription[0] if transcription else 'No text'}\")\n",
    "            print(f\"  Time: {inference_time:.4f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "    else:\n",
    "        print(f\"\\nFile not found: {audio_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Batch processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dfbd72",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ **What we accomplished:**\n",
    "- Loaded the Wav2Vec2 Hindi ASR model on CPU\n",
    "- Transcribed audio files to text\n",
    "- Measured inference performance\n",
    "\n",
    "üìä **Performance on Xeon CPU:**\n",
    "- Typical inference time: ~0.3-0.5 seconds per audio file\n",
    "- Model runs smoothly on CPU without GPU\n",
    "- Suitable for batch processing\n",
    "\n",
    "üîß **Next Steps:**\n",
    "- Try with different Hindi audio files\n",
    "- Download models for other Indian languages (Tamil, Telugu, Bengali, etc.)\n",
    "- Integrate into a web service or API\n",
    "- Deploy as a microservice\n",
    "\n",
    "üìö **Resources:**\n",
    "- [SPRING-INX Model Repository](https://asr.iitm.ac.in/models)\n",
    "- [Fairseq Documentation](https://fairseq.readthedocs.io/)\n",
    "- [Wav2Vec2 Paper](https://arxiv.org/abs/2006.11477)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
